{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNi6Cni6KQuuRtxQIpKDpIL"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Decision Trees: Introduction**"
      ],
      "metadata": {
        "id": "Mq_okpUeRNmC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The purpose of this notebook is to nuild from scrach Decision Tree classifer (Classification tree) using CART algorithm is learning algorithm. Decision trees are part of the foundation for Machine Learning. Although they are quite simple, they are very flexible and pop up in a very wide variety of situations.\n",
        "\n",
        "A Classification Tree makes a statement and then makes a decision on whether or not the statement is True or False. "
      ],
      "metadata": {
        "id": "w3V32NhG8F7I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import packages**"
      ],
      "metadata": {
        "id": "-MzQY5ufRerB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upUzwchX-a9q"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import copy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Definine variables**"
      ],
      "metadata": {
        "id": "NeaUx_WeRZhL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "col_popcorn = \"Love Popcorn\"\n",
        "col_soda = \"Love Soda\"\n",
        "col_age = \"Age\"\n",
        "col_target = \"Love Ice\""
      ],
      "metadata": {
        "id": "GAsGE105RPIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cont_variables = {col_popcorn : False,\n",
        "                  col_soda : False,\n",
        "                  col_age : True}"
      ],
      "metadata": {
        "id": "-Nk-qaptR46d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lYCqU-ElHlVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create dataframe**"
      ],
      "metadata": {
        "id": "KDDZGyd7SN2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "details = {col_popcorn : [\"Yes\", \"Yes\", \"No\", \"No\", \"Yes\", \"Yes\",\"No\"],\n",
        "           col_soda : [\"Yes\", \"No\", \"Yes\", \"Yes\", \"Yes\", \"No\",\"No\"],\n",
        "           col_age : [7, 12, 18, 35, 38, 50, 83],\n",
        "           col_target : [\"No\", \"No\", \"Yes\", \"Yes\", \"Yes\", \"No\",\"No\"]}\n",
        "\n",
        "df_ice = pd.DataFrame(details)"
      ],
      "metadata": {
        "id": "ntg9_d2F-6tg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_ice"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "JHNfuV0pBblS",
        "outputId": "a5b49d57-67a3-412e-f4ec-95ae9b2a340b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  Love Popcorn Love Soda  Age Love Ice\n",
              "0          Yes       Yes    7       No\n",
              "1          Yes        No   12       No\n",
              "2           No       Yes   18      Yes\n",
              "3           No       Yes   35      Yes\n",
              "4          Yes       Yes   38      Yes\n",
              "5          Yes        No   50       No\n",
              "6           No        No   83       No"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b6b6f10f-9982-4fc6-a747-8e5411fa0378\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Love Popcorn</th>\n",
              "      <th>Love Soda</th>\n",
              "      <th>Age</th>\n",
              "      <th>Love Ice</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>7</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>12</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>18</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>35</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>38</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>50</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>83</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b6b6f10f-9982-4fc6-a747-8e5411fa0378')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b6b6f10f-9982-4fc6-a747-8e5411fa0378 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b6b6f10f-9982-4fc6-a747-8e5411fa0378');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Gini Impurity*** is a measurement used to build Decision Trees to determine how the features of a dataset should split nodes to form the tree. More precisely, Gini impurity ranges values from 0 to 0.5., which indicates the likelihood of new, random data being misclassified if it were given a random class label according to the class distribution in the dataset."
      ],
      "metadata": {
        "id": "ufAgKI0hmNHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gini Impirity : the math**"
      ],
      "metadata": {
        "id": "kRd5Y_qnwjDk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let $X_k(X_1, X_2, ..., X_{k-1}, X_k)$ denote the features in our dataset $D$ and $Y(y_1, y_2, ..., y_{l-1}, y_l)$ contains samples from $l$ classes. The probability of a datapoint belonging to class $i$ at a given node can be denoted as $p_i$. The Gini Impurity of %D% can be defined as:\n",
        "$$Gini = 1 - \\sum_{i=1}^lp_{i}^2 $$"
      ],
      "metadata": {
        "id": "9sJO12CuwBxL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gini(label_count):\n",
        "  \"\"\"\n",
        "  Compute Gini metric which measure the degree of randomness\n",
        "  Params:\n",
        "    label_count (dict) : Consist of a column feature values and their count\n",
        "  \"\"\"\n",
        "  prob_sum = 0\n",
        "  total_count = sum(label_count.values())\n",
        "  for v in label_count.values():\n",
        "    p_i = (v/total_count)**2\n",
        "    prob_sum += p_i\n",
        "  return 1 - prob_sum"
      ],
      "metadata": {
        "id": "q4D9fnayBcgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Gini Impurity of 0 means there’s no impurity, so the data in our node is completely pure. Completely pure means the elements in the node belong to only one category."
      ],
      "metadata": {
        "id": "lF6jjx0T0Xs_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://cdn-images-1.medium.com/max/730/1*3SMk52FU2a3TrDysEu7dkQ.png\" align=\"center\" alt=\"sigmoid function\" width=\"500\"/><br>"
      ],
      "metadata": {
        "id": "0jhkIy_924gG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gini_impurity(df, feature, target, is_continuous=False):\n",
        "  \"\"\"\n",
        "  Compute Gini Impurity for a given feature\n",
        "  Params :\n",
        "    df (DataFrame) : Input DataFrame\n",
        "    feature (str) : Feature variable\n",
        "    Target (str) : Target variable\n",
        "    is_continuous (boolean) : Default is False. Says whether a not the feature variable to use in tree node is continuous\n",
        "  Returns:\n",
        "    gini (float) : Computed gini impurity \n",
        "  \"\"\"\n",
        "  if is_continuous:\n",
        "    gini, split_value = handle_continuous_variable(df, feature, target)\n",
        "    return gini, split_value\n",
        "  else:\n",
        "    return handle_categorical_variable(df, feature, target)\n",
        "\n",
        "def handle_categorical_variable(df, feature, target):\n",
        "  \"\"\"\n",
        "  Compute Gini Impurity for a categorical variable\n",
        "  Params :\n",
        "    df (DataFrame) : Input DataFrame\n",
        "    feature (str) : Feature variable\n",
        "    Target (str) : Target variable\n",
        "  Returns:\n",
        "    gini (float) : Computed gini impurity\n",
        "  \"\"\"\n",
        "  gini = 0\n",
        "  label_dict = df[feature].value_counts().to_dict()\n",
        "  total_count = sum(label_dict.values())\n",
        "  for k,v in label_dict.items():\n",
        "    df_subset = df[df[feature]==k]\n",
        "    k_dict = df_subset[target].value_counts().to_dict()\n",
        "    n_k = sum(k_dict.values())\n",
        "    gini += (n_k/total_count)*compute_gini(k_dict)\n",
        "  return gini\n",
        "\n",
        "def handle_continuous_variable(df, feature, target):\n",
        "  \"\"\"\n",
        "  Compute Gini Impurity for a continuous variable\n",
        "  Params :\n",
        "    df (DataFrame) : Input DataFrame\n",
        "    feature (str) : Feature variable\n",
        "    Target (str) : Target variable\n",
        "  Returns:\n",
        "    gini (float) : Computed gini impurity\n",
        "  \"\"\"\n",
        "  gini_dict = {}\n",
        "  df = df.sort_values([feature])\n",
        "  total_count = len(df)\n",
        "  values = df[feature].to_list()\n",
        "  avg_values = [(values[i]+values[i+1])/2 for i in range(len(values)-1)]\n",
        "  for v in avg_values:\n",
        "    df_subset_left = df[df[feature]<=v][target]\n",
        "    label_count_left = df_subset_left.value_counts().to_dict()\n",
        "    n_k_left = sum(label_count_left.values())\n",
        "\n",
        "    df_subset_right = df[df[feature]>v][target]\n",
        "    label_count_right = df_subset_right.value_counts().to_dict()\n",
        "    n_k_right = sum(label_count_right.values())\n",
        "    \n",
        "    gini = (n_k_left/total_count)*compute_gini(label_count_left) + (n_k_right/total_count)*compute_gini(label_count_right)\n",
        "    gini_dict[v] = gini\n",
        "\n",
        "  candidate_value = min(gini_dict, key=gini_dict.get)  \n",
        "  return gini_dict[candidate_value], candidate_value"
      ],
      "metadata": {
        "id": "nxrnO3hMIp2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecisionTreeClassifier():\n",
        "  \"\"\"\n",
        "  Class to build a decision tree classifier\n",
        "  Params:\n",
        "    df (DataFrame) : Input DataFrame\n",
        "    target (str) : The label/target column name\n",
        "    attributes_dict (dict) : Dictionary containing features as keys and a boolean as value indicating whether or not the future is a continuous variable\n",
        "    max_depth (int) : Maximum depth of the tree. Parameter to avoid overfitting\n",
        "  \"\"\"\n",
        "  def __init__(self, df, target, attributes_dict, max_depth, min_samples_split):\n",
        "    self.df = df\n",
        "    self.target = target\n",
        "    self.attributes_dict = attributes_dict\n",
        "    self.max_depth = max_depth\n",
        "    self.min_samples_split = min_samples_split\n",
        "    self.depth = 0\n",
        "\n",
        "  def build_tree(self, dataframe, tree={}):\n",
        "    \"\"\"\n",
        "    Build the decision tree. The function returns a dictionary containing all nodes and leaves with the predicted labls\n",
        "    \"\"\"\n",
        "    if is_leaf(dataframe, self.target):\n",
        "      return f\"{self.target} prediction : {next(iter(set(list(dataframe[self.target]))))}\"\n",
        "    elif stopping_criteria(dataframe, self.depth, self.max_depth, self.min_samples_split):\n",
        "      label = dataframe[self.target].value_counts().to_dict()\n",
        "      return f\"{self.target} prediction : {max(label, key=label.get)}\" \n",
        "    else:\n",
        "      self.depth = self.depth + 1\n",
        "      #Find best candidate for  Node\n",
        "      colname, gini, split_value = find_best_split(dataframe, self.attributes_dict, self.target)\n",
        "      #print(f\"Best column split {colname}\")\n",
        "      question = node_feature(self.attributes_dict, colname, default=split_value)\n",
        "      df_left, df_right = get_right_left_branches(dataframe, self.attributes_dict, colname, self.target, split_value)\n",
        "      self.attributes_dict.pop(colname, None)\n",
        "      print(f\"Attribute dict after {colname} removal is : {self.attributes_dict}\")\n",
        "      tree = {\"Split Question\" : question,\n",
        "              \"Feature\" : colname,\n",
        "              \"Gini Impurity\" : gini}\n",
        "      tree[\"left\"] = self.build_tree(df_left)\n",
        "      tree[\"right\"] = self.build_tree(df_right)\n",
        "      return tree\n",
        "\n",
        "def is_leaf(df, target):\n",
        "  \"\"\"\n",
        "  Check whether or not we have a Leaf Node\n",
        "  \"\"\"\n",
        "  if df is not None:\n",
        "    label = df[target].value_counts().to_dict()\n",
        "    return len(label)==1\n",
        "  else:\n",
        "    return None\n",
        "\n",
        "def stopping_criteria(df, depth, max_depth, min_samples_split):\n",
        "  \"\"\"\n",
        "  Evaluate the stopping criteria. \n",
        "  If, for example, there are fewer samples than the minimum required samples per split remaining\n",
        "  \"\"\"\n",
        "  if df is not None:\n",
        "    n_sample = len(df)\n",
        "    if n_sample < min_samples_split or depth > max_depth:\n",
        "      return True\n",
        "    return False\n",
        "  else :\n",
        "    return None\n",
        "\n",
        "def get_right_left_branches(df, attr_dict, attr_name, target, default=None):\n",
        "  \"\"\"\n",
        "  Partition data into right and left tree branches \n",
        "  Params:\n",
        "    df (DataFrame) : Input dataframe\n",
        "    attr_dict (dict) : Attribute type dictionary defined at the beginning of this notebook\n",
        "    default (float) : Default is None. This parameter is for continuous feature variable value to be used to split node\n",
        "  Returns\n",
        "    df_left (DataFrame) : Input data filtered to be used at tree left branch\n",
        "    df_right (DataFrame) : Input data filtered to be used at tree right branch\n",
        "  \"\"\"\n",
        "  if attr_dict[attr_name]:\n",
        "    df_left = df[df[attr_name]<=default]\n",
        "    df_right = df[df[attr_name]>default]\n",
        "    return df_left, df_right\n",
        "  else:\n",
        "    label = list(df[attr_name].value_counts().to_dict().keys())\n",
        "    if len(label)>2:\n",
        "      val = best_cat_value_split(df, attr_name, label, target)\n",
        "      print(f\"Best split value for attribute {attr_name} is : {val}\")\n",
        "      df_left = df[df[attr_name]==val]\n",
        "      df_right = df[df[attr_name]!=val]\n",
        "      return df_left, df_right\n",
        "    elif len(label)==2:\n",
        "      print(f\"Best split value is ==> {attr_name}\")\n",
        "      df_left = df[df[attr_name]==label[0]]\n",
        "      df_right = df[df[attr_name]==label[1]]\n",
        "      return df_left, df_right\n",
        "    else:\n",
        "      return df[df[attr_name]==label[0]], None\n",
        "\n",
        "def best_cat_value_split(df, colname, label, target):\n",
        "  \"\"\"\n",
        "  Get the feature value that has the lowest gini impurity if the column contains more than 2 values\n",
        "  \"\"\"\n",
        "  gini_dict = {}\n",
        "  for val in label:\n",
        "    df_subset = df[df[colname]==val]\n",
        "    k_dict = df_subset[target].value_counts().to_dict()\n",
        "    gini_dict[val] = compute_gini(k_dict)\n",
        "  return min(gini_dict, key=gini_dict.get)\n",
        "\n",
        "def find_best_split(df, attributes, target):\n",
        "  \"\"\"\n",
        "  Find the best attribute on which to build the tree\n",
        "  \"\"\"\n",
        "  gini_dict = {}\n",
        "  continuous_dict = {}\n",
        "  con_split_value = 0\n",
        "  for attr, value in attributes.items():\n",
        "    if value:\n",
        "      gini, feature_value = compute_gini_impurity(df, attr, target, is_continuous=value)\n",
        "      gini_dict[attr] = gini\n",
        "      continuous_dict[attr] = feature_value\n",
        "    else:\n",
        "      gini = compute_gini_impurity(df, attr, target)\n",
        "      gini_dict[attr] = gini\n",
        "\n",
        "  best_candidate = min(gini_dict, key=gini_dict.get)\n",
        "  if attributes[best_candidate]:\n",
        "    con_split_value = continuous_dict[best_candidate]\n",
        "  return best_candidate, gini_dict[best_candidate], con_split_value\n",
        "\n",
        "def node_feature(attributes, attr_name, default=None, condition=None):\n",
        "  \"\"\"\n",
        "  This function return a question to ask. This a question used to split the tree\n",
        "  \"\"\"\n",
        "  if not attributes[attr_name]:\n",
        "    return f\"Node feature variable is : {attr_name}\"\n",
        "  else:\n",
        "    condition = \"<=\"\n",
        "    return f\"Is {attr_name} {condition} {default}\""
      ],
      "metadata": {
        "id": "IKTEX2XbO5aB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test code**"
      ],
      "metadata": {
        "id": "qW_LG15eWYXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tree = DecisionTreeClassifier(df_ice, col_target, cont_variables, 5, 2)"
      ],
      "metadata": {
        "id": "SIIwKZmpQYFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tree.build_tree(df_ice)"
      ],
      "metadata": {
        "id": "rOeaMgjMarqB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac111320-a03d-41a8-e541-b8960cec8ddb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best split value is ==> Love Soda\n",
            "Attribute dict after Love Soda removal is : {'Love Popcorn': False, 'Age': True}\n",
            "Attribute dict after Age removal is : {'Love Popcorn': False}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Feature': 'Love Soda',\n",
              " 'Gini Impurity': 0.21428571428571427,\n",
              " 'Split Question': 'Node feature variable is : Love Soda',\n",
              " 'left': {'Feature': 'Age',\n",
              "  'Gini Impurity': 0.0,\n",
              "  'Split Question': 'Is Age <= 12.5',\n",
              "  'left': 'Love Ice prediction : No',\n",
              "  'right': 'Love Ice prediction : Yes'},\n",
              " 'right': 'Love Ice prediction : No'}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    }
  ]
}