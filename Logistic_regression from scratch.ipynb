{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In this notebook, we will be implementing logistic regression from scratch for sentiment analysis on tweets. Given a tweet, we will decide if it has a positive sentiment or a negative one. We will use ntltk tweeter dataset for the sentiment analysis task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn import metrics\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "\n",
    "Logistic regression model uses the logistic function to squeeze the output of a linear equation between 0 and 1. \n",
    "#### The logistic function \n",
    "\n",
    "$$ h(z) = \\frac{1}{1+\\exp^{-z}} \\tag{1}$$\n",
    "\n",
    "It maps the input 'z' to a value that ranges between 0 and 1, and so it can be treated as a probability. \n",
    "\n",
    "<div style=\"width:image width px; font-size:100%; text-align:center;\"><img src='../tmp2/sigmoid_plot.jpg' alt=\"alternate text\" width=\"width\" height=\"height\" style=\"width:300px;height:200px;\" /> Figure 1 </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take the example of the following regular linear regression :\n",
    "\n",
    "$$z = w_0 x_0 + w_1 x_1 + w_2 x_2 + ... w_N x_N$$\n",
    "\n",
    "Note that the `w` values are \"weights\". \n",
    "\n",
    "In logistic regression, we apply a sigmoid to the output of the obove linear regression equation.\n",
    "\n",
    "$$ h(z) = \\frac{1}{1+\\exp^{-z}}$$\n",
    "\n",
    "$$z = w_0 x_0 + w_1 x_1 + w_2 x_2 + ... w_N x_N$$\n",
    "\n",
    "#### Cost function\n",
    "The cost function used for logistic regression is the average of the log loss across all training examples:\n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m y^{(i)}\\log (h(z(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-h(z(\\theta)^{(i)}))\\tag{5} $$\n",
    "* $m$ is the number of training examples\n",
    "* $y^{(i)}$ is the actual label of the i-th training example.\n",
    "* $h(z(\\theta)^{(i)})$ is the model's prediction for the i-th training example.\n",
    "\n",
    "We can rewrite it into matrix multiplication :\n",
    "\n",
    "$$J = \\frac{-1}{m} \\times \\left(\\mathbf{y}^T \\cdot log(\\mathbf{h}) + \\mathbf{(1-y)}^T \\cdot log(\\mathbf{1-h}) \\right)$$\n",
    "\n",
    "The loss function for a single training example is\n",
    "$$ Loss = -1 \\times \\left( y^{(i)}\\log (h(z(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-h(z(\\theta)^{(i)})) \\right)$$\n",
    "\n",
    "#### Update weights\n",
    "\n",
    "To update the weight vector $w$, we will apply **gradient descent** to iteratively improve the model's predictions.  \n",
    "\n",
    "The gradient of the cost function $J$ with respect to one of the weights $w_j$ is:\n",
    "\n",
    "$$\\nabla_{w_j}J(w) = \\frac{1}{m} \\sum_{i=1}^m(h^{(i)}-y^{(i)})x_j \\tag{5}$$\n",
    "* 'i' is the index across all 'm' training examples.\n",
    "* 'j' is the index of the weight $w_j$, so $x_j$ is the feature associated with weight $w_j$\n",
    "\n",
    "* To update the weight $w_j$, we adjust it by subtracting a fraction of the gradient determined by $\\alpha$:\n",
    "\n",
    "$$w_j = w_j - \\alpha \\times \\nabla_{w_j}J(w) $$\n",
    "\n",
    "* The learning rate $\\alpha$ is a value that we choose to control how big a single update will be.\n",
    "\n",
    "Matrix multiplication :\n",
    "\n",
    "$$\\mathbf{w} = \\mathbf{w} - \\frac{\\alpha}{m} \\times \\left( \\mathbf{x}^T \\cdot \\left( \\mathbf{h-y} \\right) \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sigmoid(z):\n",
    "    h = 1/(1 + np.exp(-z))\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRECT!\n"
     ]
    }
   ],
   "source": [
    "if (compute_sigmoid(4.92) == 0.9927537604041685):\n",
    "    print('CORRECT!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_descent(x, y, weight, alpha, n_iters):\n",
    "    '''\n",
    "    Input:\n",
    "        x: matrix of features which is (m,n+1)\n",
    "        y: corresponding labels of the input matrix x, dimensions (m,1)\n",
    "        theta: weight vector of dimension (n+1,1)\n",
    "        alpha: learning rate\n",
    "        n_iters: number of iterations you want to train your model for\n",
    "    Output:\n",
    "        J: the final cost\n",
    "        w: your final weight vector\n",
    "    '''\n",
    "    m = x.shape[0]\n",
    "    for i in range(n_iters):\n",
    "        z = np.dot(x,weight)\n",
    "        h = compute_sigmoid(z)\n",
    "        J = -(np.dot(y.T , np.log(h)) + np.dot((1 - y).T, np.log(1-h)))/m\n",
    "        \n",
    "         # update the weights theta\n",
    "        weight = weight - (1/m) * alpha * np.dot(np.transpose(x), (h - y)) \n",
    "                                 \n",
    "    J = float(J)\n",
    "    \n",
    "    return J, weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /Users/diouladoucoure/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/diouladoucoure/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import dataset\n",
    "nltk.download('twitter_samples')\n",
    "\n",
    "#import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import twitter_samples\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing\n",
    "* The `twitter_samples` contains subsets of 5,000 positive tweets, 5,000 negative tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the set of positive and negative tweets\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example of positive tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_positive_tweets[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Train - test split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into two pieces, one for training and one for testing (validation set) \n",
    "test_pos = all_positive_tweets[4000:]\n",
    "train_pos = all_positive_tweets[:4000]\n",
    "test_neg = all_negative_tweets[4000:]\n",
    "train_neg = all_negative_tweets[:4000]\n",
    "\n",
    "train_x = train_pos + train_neg \n",
    "test_x = test_pos + test_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine positive and negative labels\n",
    "train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n",
    "test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_y.shape = (8000, 1)\n",
      "test_y.shape = (2000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape train and test sets\n",
    "print(\"train_y.shape = \" + str(train_y.shape))\n",
    "print(\"test_y.shape = \" + str(test_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Cleaning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing is one of the critical steps in any machine learning project. It includes cleaning and formatting the data before feeding into a machine learning algorithm. For NLP, the preprocessing steps are comprised of the following tasks:\n",
    "* Tokenizing the string\n",
    "* Lowercasing\n",
    "* Removing stop words and punctuation\n",
    "* Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(tweet):\n",
    "    \"\"\"Process tweet function.\n",
    "    Input:\n",
    "        tweet: a string containing a tweet\n",
    "    Output:\n",
    "        tweets_clean: a list of words containing the processed tweet\n",
    "\n",
    "    \"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and  # remove stopwords\n",
    "                word not in string.punctuation):  # remove punctuation\n",
    "            # tweets_clean.append(word)\n",
    "            stem_word = stemmer.stem(word)  # stemming word\n",
    "            tweets_clean.append(stem_word)\n",
    "\n",
    "    return tweets_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an example of a positive tweet: \n",
      " #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
      "\n",
      "This is an example of the processed version of the tweet: \n",
      " ['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "# test the function below\n",
    "print('This is an example of a positive tweet: \\n', train_x[0])\n",
    "print('\\nThis is an example of the processed version of the tweet: \\n', process_tweet(train_x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Word frequency\n",
    "\n",
    "We are going to build a frequency dictionary. The key is the tuple (word, label), such as (\"happy\",1) or (\"happy\",0). The value stored for each key is the count of how many times the word \"happy\" was associated with a positive label, or how many times \"happy\" was associated with a negative label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_freqs(tweets, labels):\n",
    "    \"\"\"\n",
    "    tweets : a list of tweets\n",
    "    labels : is a list of labels associated to each tweet \n",
    "    \"\"\"\n",
    "    freq = {}\n",
    "    for label, tweet in zip(labels, tweets):\n",
    "        for word in process_tweet(tweet):\n",
    "            if (word, label) in freq:\n",
    "                freq[(word, label)] += 1\n",
    "            else :\n",
    "                freq[(word, label)] = 1\n",
    "    return freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create frequency dictionary\n",
    "freqs = get_freqs(train_x, np.squeeze(train_y).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('followfriday', 1.0): 23,\n",
       " ('top', 1.0): 30,\n",
       " ('engag', 1.0): 7,\n",
       " ('member', 1.0): 14,\n",
       " ('commun', 1.0): 27,\n",
       " ('week', 1.0): 72,\n",
       " (':)', 1.0): 2847,\n",
       " ('hey', 1.0): 60,\n",
       " ('jame', 1.0): 7,\n",
       " ('odd', 1.0): 2,\n",
       " (':/', 1.0): 5,\n",
       " ('pleas', 1.0): 80,\n",
       " ('call', 1.0): 27,\n",
       " ('contact', 1.0): 4,\n",
       " ('centr', 1.0): 1,\n",
       " ('02392441234', 1.0): 1,\n",
       " ('abl', 1.0): 6,\n",
       " ('assist', 1.0): 1,\n",
       " ('mani', 1.0): 28,\n",
       " ('thank', 1.0): 504,\n",
       " ('listen', 1.0): 14,\n",
       " ('last', 1.0): 39,\n",
       " ('night', 1.0): 55,\n",
       " ('bleed', 1.0): 2,\n",
       " ('amaz', 1.0): 41,\n",
       " ('track', 1.0): 5,\n",
       " ('scotland', 1.0): 2,\n",
       " ('congrat', 1.0): 15,\n",
       " ('yeaaah', 1.0): 1,\n",
       " ('yipppi', 1.0): 1,\n",
       " ('accnt', 1.0): 2,\n",
       " ('verifi', 1.0): 2,\n",
       " ('rqst', 1.0): 1,\n",
       " ('succeed', 1.0): 1,\n",
       " ('got', 1.0): 57,\n",
       " ('blue', 1.0): 8,\n",
       " ('tick', 1.0): 1,\n",
       " ('mark', 1.0): 1,\n",
       " ('fb', 1.0): 4,\n",
       " ('profil', 1.0): 2,\n",
       " ('15', 1.0): 4,\n",
       " ('day', 1.0): 187,\n",
       " ('one', 1.0): 90,\n",
       " ('irresist', 1.0): 2,\n",
       " ('flipkartfashionfriday', 1.0): 16,\n",
       " ('like', 1.0): 187,\n",
       " ('keep', 1.0): 55,\n",
       " ('love', 1.0): 336,\n",
       " ('custom', 1.0): 4,\n",
       " ('wait', 1.0): 55,\n",
       " ('long', 1.0): 27,\n",
       " ('hope', 1.0): 113,\n",
       " ('enjoy', 1.0): 57,\n",
       " ('happi', 1.0): 161,\n",
       " ('friday', 1.0): 91,\n",
       " ('lwwf', 1.0): 1,\n",
       " ('second', 1.0): 8,\n",
       " ('thought', 1.0): 21,\n",
       " ('‚Äô', 1.0): 17,\n",
       " ('enough', 1.0): 16,\n",
       " ('time', 1.0): 100,\n",
       " ('dd', 1.0): 1,\n",
       " ('new', 1.0): 111,\n",
       " ('short', 1.0): 6,\n",
       " ('enter', 1.0): 9,\n",
       " ('system', 1.0): 2,\n",
       " ('sheep', 1.0): 1,\n",
       " ('must', 1.0): 14,\n",
       " ('buy', 1.0): 9,\n",
       " ('jgh', 1.0): 4,\n",
       " ('go', 1.0): 120,\n",
       " ('bayan', 1.0): 1,\n",
       " (':D', 1.0): 498,\n",
       " ('bye', 1.0): 5,\n",
       " ('act', 1.0): 6,\n",
       " ('mischiev', 1.0): 1,\n",
       " ('etl', 1.0): 1,\n",
       " ('layer', 1.0): 1,\n",
       " ('in-hous', 1.0): 1,\n",
       " ('wareh', 1.0): 1,\n",
       " ('app', 1.0): 12,\n",
       " ('katamari', 1.0): 1,\n",
       " ('well', 1.0): 66,\n",
       " ('‚Ä¶', 1.0): 31,\n",
       " ('name', 1.0): 12,\n",
       " ('impli', 1.0): 1,\n",
       " (':p', 1.0): 103,\n",
       " ('influenc', 1.0): 16,\n",
       " ('big', 1.0): 27,\n",
       " ('...', 1.0): 227,\n",
       " ('juici', 1.0): 3,\n",
       " ('selfi', 1.0): 11,\n",
       " ('follow', 1.0): 319,\n",
       " ('perfect', 1.0): 17,\n",
       " ('alreadi', 1.0): 19,\n",
       " ('know', 1.0): 120,\n",
       " (\"what'\", 1.0): 14,\n",
       " ('great', 1.0): 134,\n",
       " ('opportun', 1.0): 17,\n",
       " ('junior', 1.0): 2,\n",
       " ('triathlet', 1.0): 1,\n",
       " ('age', 1.0): 2,\n",
       " ('12', 1.0): 5,\n",
       " ('13', 1.0): 5,\n",
       " ('gatorad', 1.0): 1,\n",
       " ('seri', 1.0): 4,\n",
       " ('get', 1.0): 164,\n",
       " ('entri', 1.0): 3,\n",
       " ('lay', 1.0): 3,\n",
       " ('greet', 1.0): 4,\n",
       " ('card', 1.0): 6,\n",
       " ('rang', 1.0): 2,\n",
       " ('print', 1.0): 3,\n",
       " ('today', 1.0): 86,\n",
       " ('job', 1.0): 34,\n",
       " (':-)', 1.0): 543,\n",
       " (\"friend'\", 1.0): 3,\n",
       " ('lunch', 1.0): 3,\n",
       " ('yummm', 1.0): 1,\n",
       " ('nostalgia', 1.0): 1,\n",
       " ('tb', 1.0): 1,\n",
       " ('ku', 1.0): 1,\n",
       " ('id', 1.0): 8,\n",
       " ('conflict', 1.0): 1,\n",
       " ('help', 1.0): 37,\n",
       " (\"here'\", 1.0): 20,\n",
       " ('screenshot', 1.0): 2,\n",
       " ('work', 1.0): 88,\n",
       " ('hi', 1.0): 154,\n",
       " ('liv', 1.0): 2,\n",
       " ('hello', 1.0): 49,\n",
       " ('need', 1.0): 62,\n",
       " ('someth', 1.0): 25,\n",
       " ('u', 1.0): 136,\n",
       " ('fm', 1.0): 2,\n",
       " ('twitter', 1.0): 25,\n",
       " ('‚Äî', 1.0): 22,\n",
       " ('sure', 1.0): 37,\n",
       " ('thing', 1.0): 48,\n",
       " ('dm', 1.0): 34,\n",
       " ('x', 1.0): 50,\n",
       " (\"i'v\", 1.0): 25,\n",
       " ('heard', 1.0): 9,\n",
       " ('four', 1.0): 5,\n",
       " ('season', 1.0): 5,\n",
       " ('pretti', 1.0): 17,\n",
       " ('dope', 1.0): 2,\n",
       " ('penthous', 1.0): 1,\n",
       " ('obv', 1.0): 1,\n",
       " ('gobigorgohom', 1.0): 1,\n",
       " ('fun', 1.0): 45,\n",
       " (\"y'all\", 1.0): 3,\n",
       " ('yeah', 1.0): 30,\n",
       " ('suppos', 1.0): 6,\n",
       " ('lol', 1.0): 48,\n",
       " ('chat', 1.0): 9,\n",
       " ('bit', 1.0): 16,\n",
       " ('youth', 1.0): 14,\n",
       " ('üíÖ', 1.0): 1,\n",
       " ('üèΩ', 1.0): 1,\n",
       " ('üíã', 1.0): 2,\n",
       " ('seen', 1.0): 6,\n",
       " ('year', 1.0): 33,\n",
       " ('rest', 1.0): 9,\n",
       " ('goe', 1.0): 4,\n",
       " ('quickli', 1.0): 3,\n",
       " ('bed', 1.0): 8,\n",
       " ('music', 1.0): 15,\n",
       " ('fix', 1.0): 6,\n",
       " ('dream', 1.0): 17,\n",
       " ('spiritu', 1.0): 1,\n",
       " ('ritual', 1.0): 1,\n",
       " ('festiv', 1.0): 7,\n",
       " ('n√©pal', 1.0): 1,\n",
       " ('begin', 1.0): 4,\n",
       " ('line-up', 1.0): 4,\n",
       " ('left', 1.0): 10,\n",
       " ('see', 1.0): 156,\n",
       " ('sarah', 1.0): 4,\n",
       " ('send', 1.0): 17,\n",
       " ('us', 1.0): 91,\n",
       " ('email', 1.0): 22,\n",
       " ('bitsy@bitdefender.com', 1.0): 1,\n",
       " (\"we'll\", 1.0): 12,\n",
       " ('asap', 1.0): 5,\n",
       " ('kik', 1.0): 16,\n",
       " ('hatessuc', 1.0): 1,\n",
       " ('32429', 1.0): 1,\n",
       " ('kikm', 1.0): 1,\n",
       " ('lgbt', 1.0): 2,\n",
       " ('tinder', 1.0): 1,\n",
       " ('nsfw', 1.0): 1,\n",
       " ('akua', 1.0): 1,\n",
       " ('cumshot', 1.0): 1,\n",
       " ('come', 1.0): 63,\n",
       " ('hous', 1.0): 5,\n",
       " ('nsn_supplement', 1.0): 1,\n",
       " ('effect', 1.0): 2,\n",
       " ('press', 1.0): 1,\n",
       " ('releas', 1.0): 11,\n",
       " ('distribut', 1.0): 1,\n",
       " ('result', 1.0): 2,\n",
       " ('link', 1.0): 13,\n",
       " ('remov', 1.0): 3,\n",
       " ('pressreleas', 1.0): 1,\n",
       " ('newsdistribut', 1.0): 1,\n",
       " ('bam', 1.0): 44,\n",
       " ('bestfriend', 1.0): 50,\n",
       " ('lot', 1.0): 80,\n",
       " ('warsaw', 1.0): 44,\n",
       " ('<3', 1.0): 118,\n",
       " ('x46', 1.0): 1,\n",
       " ('everyon', 1.0): 45,\n",
       " ('watch', 1.0): 32,\n",
       " ('documentari', 1.0): 1,\n",
       " ('earthl', 1.0): 1,\n",
       " ('youtub', 1.0): 8,\n",
       " ('support', 1.0): 25,\n",
       " ('buuut', 1.0): 1,\n",
       " ('oh', 1.0): 44,\n",
       " ('look', 1.0): 109,\n",
       " ('forward', 1.0): 20,\n",
       " ('visit', 1.0): 25,\n",
       " ('next', 1.0): 37,\n",
       " ('letsgetmessi', 1.0): 1,\n",
       " ('jo', 1.0): 1,\n",
       " ('make', 1.0): 69,\n",
       " ('feel', 1.0): 33,\n",
       " ('better', 1.0): 40,\n",
       " ('never', 1.0): 31,\n",
       " ('anyon', 1.0): 7,\n",
       " ('kpop', 1.0): 1,\n",
       " ('flesh', 1.0): 1,\n",
       " ('good', 1.0): 191,\n",
       " ('girl', 1.0): 34,\n",
       " ('best', 1.0): 49,\n",
       " ('wish', 1.0): 29,\n",
       " ('reason', 1.0): 10,\n",
       " ('epic', 1.0): 1,\n",
       " ('soundtrack', 1.0): 1,\n",
       " ('shout', 1.0): 11,\n",
       " ('ad', 1.0): 10,\n",
       " ('video', 1.0): 29,\n",
       " ('playlist', 1.0): 5,\n",
       " ('would', 1.0): 70,\n",
       " ('dear', 1.0): 15,\n",
       " ('jordan', 1.0): 1,\n",
       " ('okay', 1.0): 31,\n",
       " ('fake', 1.0): 1,\n",
       " ('gameplay', 1.0): 1,\n",
       " (';)', 1.0): 22,\n",
       " ('haha', 1.0): 44,\n",
       " ('im', 1.0): 38,\n",
       " ('kid', 1.0): 13,\n",
       " ('stuff', 1.0): 11,\n",
       " ('exactli', 1.0): 5,\n",
       " ('product', 1.0): 11,\n",
       " ('line', 1.0): 6,\n",
       " ('etsi', 1.0): 1,\n",
       " ('shop', 1.0): 12,\n",
       " ('check', 1.0): 38,\n",
       " ('vacat', 1.0): 5,\n",
       " ('recharg', 1.0): 1,\n",
       " ('normal', 1.0): 5,\n",
       " ('charger', 1.0): 2,\n",
       " ('asleep', 1.0): 7,\n",
       " ('talk', 1.0): 37,\n",
       " ('sooo', 1.0): 6,\n",
       " ('someon', 1.0): 29,\n",
       " ('text', 1.0): 12,\n",
       " ('ye', 1.0): 60,\n",
       " ('bet', 1.0): 6,\n",
       " (\"he'll\", 1.0): 2,\n",
       " ('fit', 1.0): 2,\n",
       " ('hear', 1.0): 24,\n",
       " ('speech', 1.0): 1,\n",
       " ('piti', 1.0): 2,\n",
       " ('green', 1.0): 2,\n",
       " ('garden', 1.0): 5,\n",
       " ('midnight', 1.0): 1,\n",
       " ('sun', 1.0): 6,\n",
       " ('beauti', 1.0): 45,\n",
       " ('canal', 1.0): 1,\n",
       " ('dasvidaniya', 1.0): 1,\n",
       " ('till', 1.0): 16,\n",
       " ('scout', 1.0): 1,\n",
       " ('sg', 1.0): 1,\n",
       " ('futur', 1.0): 9,\n",
       " ('wlan', 1.0): 1,\n",
       " ('pro', 1.0): 4,\n",
       " ('confer', 1.0): 1,\n",
       " ('asia', 1.0): 1,\n",
       " ('chang', 1.0): 20,\n",
       " ('lollipop', 1.0): 1,\n",
       " ('üç≠', 1.0): 1,\n",
       " ('nez', 1.0): 1,\n",
       " ('agnezmo', 1.0): 1,\n",
       " ('oley', 1.0): 1,\n",
       " ('mama', 1.0): 1,\n",
       " ('stand', 1.0): 6,\n",
       " ('stronger', 1.0): 1,\n",
       " ('god', 1.0): 14,\n",
       " ('misti', 1.0): 1,\n",
       " ('babi', 1.0): 17,\n",
       " ('cute', 1.0): 21,\n",
       " ('woohoo', 1.0): 3,\n",
       " (\"can't\", 1.0): 31,\n",
       " ('sign', 1.0): 9,\n",
       " ('yet', 1.0): 12,\n",
       " ('still', 1.0): 37,\n",
       " ('think', 1.0): 48,\n",
       " ('mka', 1.0): 5,\n",
       " ('liam', 1.0): 5,\n",
       " ('access', 1.0): 3,\n",
       " ('welcom', 1.0): 54,\n",
       " ('stat', 1.0): 51,\n",
       " ('arriv', 1.0): 57,\n",
       " ('1', 1.0): 60,\n",
       " ('unfollow', 1.0): 53,\n",
       " ('via', 1.0): 60,\n",
       " ('surpris', 1.0): 10,\n",
       " ('figur', 1.0): 5,\n",
       " ('happybirthdayemilybett', 1.0): 1,\n",
       " ('sweet', 1.0): 16,\n",
       " ('talent', 1.0): 4,\n",
       " ('2', 1.0): 41,\n",
       " ('plan', 1.0): 21,\n",
       " ('drain', 1.0): 1,\n",
       " ('gotta', 1.0): 4,\n",
       " ('timezon', 1.0): 1,\n",
       " ('parent', 1.0): 4,\n",
       " ('proud', 1.0): 11,\n",
       " ('least', 1.0): 14,\n",
       " ('mayb', 1.0): 17,\n",
       " ('sometim', 1.0): 11,\n",
       " ('grade', 1.0): 4,\n",
       " ('al', 1.0): 3,\n",
       " ('grand', 1.0): 4,\n",
       " ('manila_bro', 1.0): 1,\n",
       " ('chosen', 1.0): 1,\n",
       " ('let', 1.0): 57,\n",
       " ('around', 1.0): 14,\n",
       " ('..', 1.0): 100,\n",
       " ('side', 1.0): 13,\n",
       " ('world', 1.0): 23,\n",
       " ('eh', 1.0): 2,\n",
       " ('take', 1.0): 30,\n",
       " ('care', 1.0): 12,\n",
       " ('final', 1.0): 24,\n",
       " ('fuck', 1.0): 20,\n",
       " ('weekend', 1.0): 61,\n",
       " ('real', 1.0): 18,\n",
       " ('x45', 1.0): 1,\n",
       " ('join', 1.0): 21,\n",
       " ('hushedcallwithfraydo', 1.0): 1,\n",
       " ('gift', 1.0): 7,\n",
       " ('yeahhh', 1.0): 1,\n",
       " ('hushedpinwithsammi', 1.0): 2,\n",
       " ('event', 1.0): 7,\n",
       " ('might', 1.0): 21,\n",
       " ('luv', 1.0): 4,\n",
       " ('realli', 1.0): 66,\n",
       " ('appreci', 1.0): 28,\n",
       " ('share', 1.0): 41,\n",
       " ('wow', 1.0): 14,\n",
       " ('tom', 1.0): 5,\n",
       " ('gym', 1.0): 3,\n",
       " ('monday', 1.0): 7,\n",
       " ('invit', 1.0): 15,\n",
       " ('scope', 1.0): 5,\n",
       " ('friend', 1.0): 40,\n",
       " ('nude', 1.0): 1,\n",
       " ('sleep', 1.0): 35,\n",
       " ('birthday', 1.0): 53,\n",
       " ('want', 1.0): 69,\n",
       " ('t-shirt', 1.0): 2,\n",
       " ('cool', 1.0): 29,\n",
       " ('haw', 1.0): 1,\n",
       " ('phela', 1.0): 1,\n",
       " ('mom', 1.0): 7,\n",
       " ('obvious', 1.0): 1,\n",
       " ('princ', 1.0): 1,\n",
       " ('charm', 1.0): 1,\n",
       " ('stage', 1.0): 2,\n",
       " ('luck', 1.0): 26,\n",
       " ('tyler', 1.0): 1,\n",
       " ('hipster', 1.0): 1,\n",
       " ('glass', 1.0): 3,\n",
       " ('marti', 1.0): 2,\n",
       " ('glad', 1.0): 41,\n",
       " ('done', 1.0): 40,\n",
       " ('afternoon', 1.0): 7,\n",
       " ('read', 1.0): 27,\n",
       " ('kahfi', 1.0): 1,\n",
       " ('finish', 1.0): 15,\n",
       " ('ohmyg', 1.0): 1,\n",
       " ('yaya', 1.0): 3,\n",
       " ('dub', 1.0): 1,\n",
       " ('stalk', 1.0): 2,\n",
       " ('ig', 1.0): 3,\n",
       " ('gondooo', 1.0): 1,\n",
       " ('moo', 1.0): 2,\n",
       " ('tologooo', 1.0): 1,\n",
       " ('becom', 1.0): 8,\n",
       " ('detail', 1.0): 8,\n",
       " ('zzz', 1.0): 1,\n",
       " ('xx', 1.0): 33,\n",
       " ('physiotherapi', 1.0): 1,\n",
       " ('hashtag', 1.0): 3,\n",
       " ('üí™', 1.0): 1,\n",
       " ('monica', 1.0): 1,\n",
       " ('miss', 1.0): 17,\n",
       " ('sound', 1.0): 20,\n",
       " ('morn', 1.0): 68,\n",
       " (\"that'\", 1.0): 49,\n",
       " ('x43', 1.0): 1,\n",
       " ('definit', 1.0): 20,\n",
       " ('tri', 1.0): 34,\n",
       " ('tonight', 1.0): 15,\n",
       " ('took', 1.0): 7,\n",
       " ('advic', 1.0): 6,\n",
       " ('treviso', 1.0): 1,\n",
       " ('concert', 1.0): 23,\n",
       " ('citi', 1.0): 26,\n",
       " ('countri', 1.0): 22,\n",
       " (\"i'll\", 1.0): 73,\n",
       " ('start', 1.0): 56,\n",
       " ('fine', 1.0): 7,\n",
       " ('gorgeou', 1.0): 9,\n",
       " ('xo', 1.0): 2,\n",
       " ('oven', 1.0): 2,\n",
       " ('roast', 1.0): 1,\n",
       " ('garlic', 1.0): 1,\n",
       " ('oliv', 1.0): 1,\n",
       " ('oil', 1.0): 4,\n",
       " ('dri', 1.0): 4,\n",
       " ('tomato', 1.0): 1,\n",
       " ('basil', 1.0): 1,\n",
       " ('centuri', 1.0): 1,\n",
       " ('tuna', 1.0): 1,\n",
       " ('right', 1.0): 38,\n",
       " ('back', 1.0): 74,\n",
       " ('atchya', 1.0): 1,\n",
       " ('even', 1.0): 26,\n",
       " ('almost', 1.0): 8,\n",
       " ('chanc', 1.0): 3,\n",
       " ('cheer', 1.0): 17,\n",
       " ('po', 1.0): 3,\n",
       " ('ice', 1.0): 6,\n",
       " ('cream', 1.0): 6,\n",
       " ('agre', 1.0): 13,\n",
       " ('100', 1.0): 6,\n",
       " ('heheheh', 1.0): 2,\n",
       " ('that', 1.0): 10,\n",
       " ('point', 1.0): 11,\n",
       " ('stay', 1.0): 20,\n",
       " ('home', 1.0): 20,\n",
       " ('soon', 1.0): 38,\n",
       " ('promis', 1.0): 4,\n",
       " ('web', 1.0): 4,\n",
       " ('whatsapp', 1.0): 3,\n",
       " ('volta', 1.0): 1,\n",
       " ('funcionar', 1.0): 1,\n",
       " ('com', 1.0): 2,\n",
       " ('iphon', 1.0): 7,\n",
       " ('jailbroken', 1.0): 1,\n",
       " ('later', 1.0): 11,\n",
       " ('34', 1.0): 3,\n",
       " ('min', 1.0): 7,\n",
       " ('leia', 1.0): 1,\n",
       " ('appear', 1.0): 3,\n",
       " ('hologram', 1.0): 1,\n",
       " ('r2d2', 1.0): 1,\n",
       " ('w', 1.0): 16,\n",
       " ('messag', 1.0): 9,\n",
       " ('obi', 1.0): 1,\n",
       " ('wan', 1.0): 1,\n",
       " ('sit', 1.0): 7,\n",
       " ('luke', 1.0): 4,\n",
       " ('inter', 1.0): 1,\n",
       " ('3', 1.0): 26,\n",
       " ('ucl', 1.0): 1,\n",
       " ('arsen', 1.0): 2,\n",
       " ('small', 1.0): 1,\n",
       " ('team', 1.0): 24,\n",
       " ('pass', 1.0): 10,\n",
       " ('üöÇ', 1.0): 1,\n",
       " ('dewsburi', 1.0): 2,\n",
       " ('railway', 1.0): 1,\n",
       " ('station', 1.0): 4,\n",
       " ('dew', 1.0): 1,\n",
       " ('west', 1.0): 1,\n",
       " ('yorkshir', 1.0): 2,\n",
       " ('430', 1.0): 1,\n",
       " ('smh', 1.0): 2,\n",
       " ('9:25', 1.0): 1,\n",
       " ('live', 1.0): 21,\n",
       " ('strang', 1.0): 4,\n",
       " ('imagin', 1.0): 5,\n",
       " ('megan', 1.0): 1,\n",
       " ('masaantoday', 1.0): 4,\n",
       " ('a4', 1.0): 3,\n",
       " ('shweta', 1.0): 1,\n",
       " ('tripathi', 1.0): 1,\n",
       " ('5', 1.0): 15,\n",
       " ('20', 1.0): 5,\n",
       " ('kurta', 1.0): 3,\n",
       " ('half', 1.0): 6,\n",
       " ('number', 1.0): 11,\n",
       " ('wsalelov', 1.0): 14,\n",
       " ('ah', 1.0): 12,\n",
       " ('larri', 1.0): 3,\n",
       " ('anyway', 1.0): 14,\n",
       " ('kinda', 1.0): 12,\n",
       " ('goood', 1.0): 1,\n",
       " ('life', 1.0): 36,\n",
       " ('enn', 1.0): 1,\n",
       " ('could', 1.0): 25,\n",
       " ('warmup', 1.0): 1,\n",
       " ('15th', 1.0): 2,\n",
       " ('bath', 1.0): 6,\n",
       " ('dum', 1.0): 2,\n",
       " ('andar', 1.0): 1,\n",
       " ('ram', 1.0): 1,\n",
       " ('sampath', 1.0): 1,\n",
       " ('sona', 1.0): 1,\n",
       " ('mohapatra', 1.0): 1,\n",
       " ('samantha', 1.0): 1,\n",
       " ('edward', 1.0): 1,\n",
       " ('mein', 1.0): 1,\n",
       " ('tulan', 1.0): 1,\n",
       " ('razi', 1.0): 2,\n",
       " ('wah', 1.0): 2,\n",
       " ('josh', 1.0): 1,\n",
       " ('alway', 1.0): 48,\n",
       " ('smile', 1.0): 47,\n",
       " ('pictur', 1.0): 7,\n",
       " ('16.20', 1.0): 1,\n",
       " ('giveitup', 1.0): 1,\n",
       " ('given', 1.0): 3,\n",
       " ('ga', 1.0): 3,\n",
       " ('subsidi', 1.0): 1,\n",
       " ('initi', 1.0): 2,\n",
       " ('propos', 1.0): 3,\n",
       " ('delight', 1.0): 4,\n",
       " ('yesterday', 1.0): 4,\n",
       " ('x42', 1.0): 1,\n",
       " ('lmaoo', 1.0): 2,\n",
       " ('song', 1.0): 16,\n",
       " ('ever', 1.0): 19,\n",
       " ('shall', 1.0): 5,\n",
       " ('littl', 1.0): 29,\n",
       " ('throwback', 1.0): 3,\n",
       " ('outli', 1.0): 1,\n",
       " ('island', 1.0): 2,\n",
       " ('cheung', 1.0): 1,\n",
       " ('chau', 1.0): 1,\n",
       " ('mui', 1.0): 1,\n",
       " ('wo', 1.0): 1,\n",
       " ('total', 1.0): 5,\n",
       " ('differ', 1.0): 10,\n",
       " ('kfckitchentour', 1.0): 2,\n",
       " ('kitchen', 1.0): 3,\n",
       " ('clean', 1.0): 1,\n",
       " (\"i'm\", 1.0): 140,\n",
       " ('cusp', 1.0): 1,\n",
       " ('test', 1.0): 7,\n",
       " ('water', 1.0): 7,\n",
       " ('reward', 1.0): 1,\n",
       " ('arummzz', 1.0): 2,\n",
       " (\"let'\", 1.0): 18,\n",
       " ('drive', 1.0): 9,\n",
       " ('travel', 1.0): 19,\n",
       " ('yogyakarta', 1.0): 3,\n",
       " ('jeep', 1.0): 3,\n",
       " ('indonesia', 1.0): 3,\n",
       " ('instamood', 1.0): 3,\n",
       " ('wanna', 1.0): 23,\n",
       " ('skype', 1.0): 3,\n",
       " ('may', 1.0): 16,\n",
       " ('nice', 1.0): 70,\n",
       " ('friendli', 1.0): 1,\n",
       " ('pretend', 1.0): 2,\n",
       " ('film', 1.0): 8,\n",
       " ('congratul', 1.0): 9,\n",
       " ('winner', 1.0): 3,\n",
       " ('cheesydelight', 1.0): 1,\n",
       " ('contest', 1.0): 5,\n",
       " ('address', 1.0): 8,\n",
       " ('guy', 1.0): 48,\n",
       " ('market', 1.0): 5,\n",
       " ('24/7', 1.0): 1,\n",
       " ('14', 1.0): 1,\n",
       " ('hour', 1.0): 24,\n",
       " ('leav', 1.0): 11,\n",
       " ('without', 1.0): 9,\n",
       " ('delay', 1.0): 1,\n",
       " ('actual', 1.0): 13,\n",
       " ('easi', 1.0): 7,\n",
       " ('guess', 1.0): 8,\n",
       " ('train', 1.0): 7,\n",
       " ('wd', 1.0): 1,\n",
       " ('shift', 1.0): 4,\n",
       " ('engin', 1.0): 1,\n",
       " ('etc', 1.0): 2,\n",
       " ('sunburn', 1.0): 1,\n",
       " ('peel', 1.0): 2,\n",
       " ('blog', 1.0): 27,\n",
       " ('huge', 1.0): 9,\n",
       " ('warm', 1.0): 4,\n",
       " ('‚òÜ', 1.0): 3,\n",
       " ('complet', 1.0): 10,\n",
       " ('triangl', 1.0): 2,\n",
       " ('northern', 1.0): 1,\n",
       " ('ireland', 1.0): 2,\n",
       " ('sight', 1.0): 1,\n",
       " ('smthng', 1.0): 2,\n",
       " ('fr', 1.0): 3,\n",
       " ('hug', 1.0): 11,\n",
       " ('xoxo', 1.0): 3,\n",
       " ('uu', 1.0): 1,\n",
       " ('jaann', 1.0): 1,\n",
       " ('topnewfollow', 1.0): 2,\n",
       " ('connect', 1.0): 13,\n",
       " ('wonder', 1.0): 26,\n",
       " ('made', 1.0): 38,\n",
       " ('fluffi', 1.0): 1,\n",
       " ('insid', 1.0): 7,\n",
       " ('pirouett', 1.0): 1,\n",
       " ('moos', 1.0): 1,\n",
       " ('trip', 1.0): 12,\n",
       " ('philli', 1.0): 1,\n",
       " ('decemb', 1.0): 2,\n",
       " (\"i'd\", 1.0): 13,\n",
       " ('dude', 1.0): 6,\n",
       " ('x41', 1.0): 1,\n",
       " ('question', 1.0): 15,\n",
       " ('flaw', 1.0): 1,\n",
       " ('pain', 1.0): 8,\n",
       " ('negat', 1.0): 1,\n",
       " ('strength', 1.0): 2,\n",
       " ('went', 1.0): 10,\n",
       " ('solo', 1.0): 4,\n",
       " ('move', 1.0): 9,\n",
       " ('fav', 1.0): 11,\n",
       " ('nirvana', 1.0): 1,\n",
       " ('smell', 1.0): 2,\n",
       " ('teen', 1.0): 3,\n",
       " ('spirit', 1.0): 1,\n",
       " ('rip', 1.0): 3,\n",
       " ('ami', 1.0): 4,\n",
       " ('winehous', 1.0): 1,\n",
       " ('coupl', 1.0): 5,\n",
       " ('tomhiddleston', 1.0): 1,\n",
       " ('elizabetholsen', 1.0): 1,\n",
       " ('yaytheylookgreat', 1.0): 1,\n",
       " ('goodnight', 1.0): 18,\n",
       " ('vid', 1.0): 8,\n",
       " ('wake', 1.0): 10,\n",
       " ('gonna', 1.0): 16,\n",
       " ('shoot', 1.0): 5,\n",
       " ('itti', 1.0): 2,\n",
       " ('bitti', 1.0): 2,\n",
       " ('teeni', 1.0): 2,\n",
       " ('bikini', 1.0): 3,\n",
       " ('much', 1.0): 73,\n",
       " ('4th', 1.0): 4,\n",
       " ('togeth', 1.0): 6,\n",
       " ('end', 1.0): 13,\n",
       " ('xfile', 1.0): 1,\n",
       " ('content', 1.0): 3,\n",
       " ('rain', 1.0): 18,\n",
       " ('fabul', 1.0): 4,\n",
       " ('fantast', 1.0): 8,\n",
       " ('‚ô°', 1.0): 12,\n",
       " ('jb', 1.0): 1,\n",
       " ('forev', 1.0): 5,\n",
       " ('belieb', 1.0): 3,\n",
       " ('nighti', 1.0): 1,\n",
       " ('bug', 1.0): 2,\n",
       " ('bite', 1.0): 1,\n",
       " ('bracelet', 1.0): 2,\n",
       " ('idea', 1.0): 23,\n",
       " ('foundri', 1.0): 1,\n",
       " ('game', 1.0): 23,\n",
       " ('sens', 1.0): 6,\n",
       " ('pic', 1.0): 21,\n",
       " ('ef', 1.0): 1,\n",
       " ('phone', 1.0): 16,\n",
       " ('woot', 1.0): 2,\n",
       " ('derek', 1.0): 1,\n",
       " ('use', 1.0): 32,\n",
       " ('parkshar', 1.0): 1,\n",
       " ('gloucestershir', 1.0): 1,\n",
       " ('aaaahhh', 1.0): 1,\n",
       " ('man', 1.0): 16,\n",
       " ('traffic', 1.0): 2,\n",
       " ('stress', 1.0): 4,\n",
       " ('reliev', 1.0): 1,\n",
       " (\"how'r\", 1.0): 1,\n",
       " ('arbeloa', 1.0): 1,\n",
       " ('turn', 1.0): 13,\n",
       " ('17', 1.0): 2,\n",
       " ('omg', 1.0): 13,\n",
       " ('say', 1.0): 43,\n",
       " ('europ', 1.0): 1,\n",
       " ('rise', 1.0): 2,\n",
       " ('find', 1.0): 20,\n",
       " ('hard', 1.0): 9,\n",
       " ('believ', 1.0): 7,\n",
       " ('uncount', 1.0): 1,\n",
       " ('coz', 1.0): 2,\n",
       " ('unlimit', 1.0): 1,\n",
       " ('cours', 1.0): 11,\n",
       " ('teamposit', 1.0): 1,\n",
       " ('aldub', 1.0): 2,\n",
       " ('‚òï', 1.0): 3,\n",
       " ('rita', 1.0): 2,\n",
       " ('info', 1.0): 10,\n",
       " (\"we'd\", 1.0): 4,\n",
       " ('way', 1.0): 34,\n",
       " ('boy', 1.0): 13,\n",
       " ('x40', 1.0): 1,\n",
       " ('true', 1.0): 19,\n",
       " ('sethi', 1.0): 2,\n",
       " ('high', 1.0): 6,\n",
       " ('exe', 1.0): 1,\n",
       " ('skeem', 1.0): 1,\n",
       " ('saam', 1.0): 1,\n",
       " ('peopl', 1.0): 42,\n",
       " ('polit', 1.0): 2,\n",
       " ('izzat', 1.0): 1,\n",
       " ('wese', 1.0): 1,\n",
       " ('trust', 1.0): 7,\n",
       " ('khawateen', 1.0): 1,\n",
       " ('k', 1.0): 8,\n",
       " ('sath', 1.0): 2,\n",
       " ('mana', 1.0): 1,\n",
       " ('kar', 1.0): 1,\n",
       " ('deya', 1.0): 1,\n",
       " ('sort', 1.0): 7,\n",
       " ('smart', 1.0): 5,\n",
       " ('hair', 1.0): 7,\n",
       " ('tbh', 1.0): 5,\n",
       " ('jacob', 1.0): 2,\n",
       " ('g', 1.0): 7,\n",
       " ('upgrad', 1.0): 2,\n",
       " ('tee', 1.0): 2,\n",
       " ('famili', 1.0): 14,\n",
       " ('person', 1.0): 14,\n",
       " ('two', 1.0): 15,\n",
       " ('convers', 1.0): 6,\n",
       " ('onlin', 1.0): 4,\n",
       " ('mclaren', 1.0): 1,\n",
       " ('fridayfeel', 1.0): 5,\n",
       " ('tgif', 1.0): 8,\n",
       " ('squar', 1.0): 1,\n",
       " ('enix', 1.0): 1,\n",
       " ('bissmillah', 1.0): 1,\n",
       " ('ya', 1.0): 19,\n",
       " ('allah', 1.0): 3,\n",
       " (\"we'r\", 1.0): 25,\n",
       " ('socent', 1.0): 1,\n",
       " ('startup', 1.0): 2,\n",
       " ('drop', 1.0): 9,\n",
       " ('your', 1.0): 3,\n",
       " ('arnd', 1.0): 1,\n",
       " ('town', 1.0): 3,\n",
       " ('basic', 1.0): 4,\n",
       " ('piss', 1.0): 2,\n",
       " ('cup', 1.0): 4,\n",
       " ('also', 1.0): 28,\n",
       " ('terribl', 1.0): 2,\n",
       " ('complic', 1.0): 1,\n",
       " ('discuss', 1.0): 2,\n",
       " ('snapchat', 1.0): 31,\n",
       " ('lynettelow', 1.0): 1,\n",
       " ('kikmenow', 1.0): 2,\n",
       " ('snapm', 1.0): 1,\n",
       " ('hot', 1.0): 20,\n",
       " ('amazon', 1.0): 1,\n",
       " ('kikmeguy', 1.0): 2,\n",
       " ('defin', 1.0): 2,\n",
       " ('grow', 1.0): 6,\n",
       " ('sport', 1.0): 4,\n",
       " ('rt', 1.0): 9,\n",
       " ('rakyat', 1.0): 1,\n",
       " ('write', 1.0): 11,\n",
       " ('sinc', 1.0): 11,\n",
       " ('mention', 1.0): 18,\n",
       " ('fli', 1.0): 5,\n",
       " ('fish', 1.0): 3,\n",
       " ('promot', 1.0): 3,\n",
       " ('post', 1.0): 16,\n",
       " ('cyber', 1.0): 1,\n",
       " ('ourdaughtersourprid', 1.0): 3,\n",
       " ('mypapamyprid', 1.0): 2,\n",
       " ('papa', 1.0): 1,\n",
       " ('coach', 1.0): 2,\n",
       " ('posit', 1.0): 3,\n",
       " ('kha', 1.0): 1,\n",
       " ('atleast', 1.0): 2,\n",
       " ('x39', 1.0): 1,\n",
       " ('mango', 1.0): 1,\n",
       " (\"lassi'\", 1.0): 1,\n",
       " (\"monty'\", 1.0): 1,\n",
       " ('marvel', 1.0): 2,\n",
       " ('though', 1.0): 16,\n",
       " ('suspect', 1.0): 3,\n",
       " ('meant', 1.0): 2,\n",
       " ('24', 1.0): 3,\n",
       " ('hr', 1.0): 2,\n",
       " ('touch', 1.0): 7,\n",
       " ('kepler', 1.0): 3,\n",
       " ('452b', 1.0): 4,\n",
       " ('chalna', 1.0): 1,\n",
       " ('hai', 1.0): 7,\n",
       " ('thankyou', 1.0): 12,\n",
       " ('hazel', 1.0): 1,\n",
       " ('food', 1.0): 6,\n",
       " ('brooklyn', 1.0): 1,\n",
       " ('pta', 1.0): 2,\n",
       " ('awak', 1.0): 8,\n",
       " ('okayi', 1.0): 2,\n",
       " ('awww', 1.0): 12,\n",
       " ('ha', 1.0): 18,\n",
       " ('doc', 1.0): 1,\n",
       " ('splendid', 1.0): 1,\n",
       " ('spam', 1.0): 1,\n",
       " ('folder', 1.0): 1,\n",
       " ('amount', 1.0): 1,\n",
       " ('nigeria', 1.0): 1,\n",
       " ('claim', 1.0): 1,\n",
       " ('rted', 1.0): 1,\n",
       " ('leg', 1.0): 3,\n",
       " ('hurt', 1.0): 4,\n",
       " ('bad', 1.0): 14,\n",
       " ('mine', 1.0): 11,\n",
       " ('saturday', 1.0): 5,\n",
       " ('thaaank', 1.0): 1,\n",
       " ('puhon', 1.0): 1,\n",
       " ('happinesss', 1.0): 1,\n",
       " ('tnc', 1.0): 1,\n",
       " ('prior', 1.0): 1,\n",
       " ('notif', 1.0): 2,\n",
       " ('fat', 1.0): 1,\n",
       " ('co', 1.0): 1,\n",
       " ('probabl', 1.0): 7,\n",
       " ('ate', 1.0): 4,\n",
       " ('yuna', 1.0): 2,\n",
       " ('tamesid', 1.0): 1,\n",
       " ('¬¥', 1.0): 3,\n",
       " ('googl', 1.0): 5,\n",
       " ('account', 1.0): 17,\n",
       " ('scouser', 1.0): 1,\n",
       " ('everyth', 1.0): 10,\n",
       " ('zoe', 1.0): 1,\n",
       " ('mate', 1.0): 5,\n",
       " ('liter', 1.0): 5,\n",
       " (\"they'r\", 1.0): 10,\n",
       " ('samee', 1.0): 1,\n",
       " ('edgar', 1.0): 1,\n",
       " ('updat', 1.0): 12,\n",
       " ('log', 1.0): 3,\n",
       " ('bring', 1.0): 12,\n",
       " ('abe', 1.0): 1,\n",
       " ('meet', 1.0): 26,\n",
       " ('x38', 1.0): 1,\n",
       " ('sigh', 1.0): 3,\n",
       " ('dreamili', 1.0): 1,\n",
       " ('pout', 1.0): 1,\n",
       " ('eye', 1.0): 12,\n",
       " ('quacketyquack', 1.0): 6,\n",
       " ('funni', 1.0): 15,\n",
       " ('happen', 1.0): 13,\n",
       " ('phil', 1.0): 1,\n",
       " ('em', 1.0): 2,\n",
       " ('del', 1.0): 1,\n",
       " ('rodder', 1.0): 1,\n",
       " ('els', 1.0): 8,\n",
       " ('play', 1.0): 37,\n",
       " ('newest', 1.0): 1,\n",
       " ('gamejam', 1.0): 1,\n",
       " ('irish', 1.0): 2,\n",
       " ('literatur', 1.0): 2,\n",
       " ('inaccess', 1.0): 2,\n",
       " (\"kareena'\", 1.0): 2,\n",
       " ('fan', 1.0): 21,\n",
       " ('brain', 1.0): 10,\n",
       " ('dot', 1.0): 8,\n",
       " ('braindot', 1.0): 8,\n",
       " ('fair', 1.0): 4,\n",
       " ('rush', 1.0): 1,\n",
       " ('either', 1.0): 10,\n",
       " ('brandi', 1.0): 1,\n",
       " ('18', 1.0): 5,\n",
       " ('carniv', 1.0): 1,\n",
       " ('men', 1.0): 8,\n",
       " ('put', 1.0): 11,\n",
       " ('mask', 1.0): 2,\n",
       " ('xavier', 1.0): 1,\n",
       " ('forneret', 1.0): 1,\n",
       " ('jennif', 1.0): 1,\n",
       " ('site', 1.0): 7,\n",
       " ('free', 1.0): 31,\n",
       " ('50.000', 1.0): 3,\n",
       " ('8', 1.0): 10,\n",
       " ('ball', 1.0): 7,\n",
       " ('pool', 1.0): 5,\n",
       " ('coin', 1.0): 5,\n",
       " ('edit', 1.0): 6,\n",
       " ('trish', 1.0): 1,\n",
       " ('‚ô•', 1.0): 13,\n",
       " ('grate', 1.0): 5,\n",
       " ('three', 1.0): 8,\n",
       " ('comment', 1.0): 7,\n",
       " ('wakeup', 1.0): 1,\n",
       " ('besid', 1.0): 2,\n",
       " ('dirti', 1.0): 2,\n",
       " ('sex', 1.0): 4,\n",
       " ('lmaooo', 1.0): 1,\n",
       " ('üò§', 1.0): 2,\n",
       " ('loui', 1.0): 4,\n",
       " (\"he'\", 1.0): 10,\n",
       " ('throw', 1.0): 3,\n",
       " ('caus', 1.0): 11,\n",
       " ('inspir', 1.0): 6,\n",
       " ('ff', 1.0): 40,\n",
       " ('twoof', 1.0): 3,\n",
       " ('gr8', 1.0): 1,\n",
       " ('wkend', 1.0): 3,\n",
       " ('kind', 1.0): 22,\n",
       " ('exhaust', 1.0): 2,\n",
       " ('word', 1.0): 17,\n",
       " ('cheltenham', 1.0): 1,\n",
       " ('area', 1.0): 4,\n",
       " ('kale', 1.0): 1,\n",
       " ('crisp', 1.0): 1,\n",
       " ('ruin', 1.0): 5,\n",
       " ('x37', 1.0): 1,\n",
       " ('open', 1.0): 12,\n",
       " ('worldwid', 1.0): 2,\n",
       " ('outta', 1.0): 1,\n",
       " ('sfvbeta', 1.0): 1,\n",
       " ('vantast', 1.0): 1,\n",
       " ('xcylin', 1.0): 1,\n",
       " ('bundl', 1.0): 1,\n",
       " ('show', 1.0): 20,\n",
       " ('internet', 1.0): 2,\n",
       " ('price', 1.0): 3,\n",
       " ('realisticli', 1.0): 1,\n",
       " ('pay', 1.0): 8,\n",
       " ('net', 1.0): 1,\n",
       " ('educ', 1.0): 1,\n",
       " ('power', 1.0): 6,\n",
       " ('weapon', 1.0): 1,\n",
       " ('nelson', 1.0): 1,\n",
       " ('mandela', 1.0): 1,\n",
       " ('recent', 1.0): 8,\n",
       " ('j', 1.0): 2,\n",
       " ('chenab', 1.0): 1,\n",
       " ('flow', 1.0): 5,\n",
       " ('pakistan', 1.0): 1,\n",
       " ('incredibleindia', 1.0): 1,\n",
       " ('teenchoic', 1.0): 7,\n",
       " ('choiceinternationalartist', 1.0): 7,\n",
       " ('superjunior', 1.0): 7,\n",
       " ('caught', 1.0): 4,\n",
       " ('first', 1.0): 41,\n",
       " ('salmon', 1.0): 1,\n",
       " ('super-blend', 1.0): 1,\n",
       " ('project', 1.0): 6,\n",
       " ('youth@bipolaruk.org.uk', 1.0): 1,\n",
       " ('awesom', 1.0): 35,\n",
       " ('stream', 1.0): 12,\n",
       " ('alma', 1.0): 1,\n",
       " ('mater', 1.0): 1,\n",
       " ('highschoolday', 1.0): 1,\n",
       " ('clientvisit', 1.0): 1,\n",
       " ('faith', 1.0): 3,\n",
       " ('christian', 1.0): 1,\n",
       " ('school', 1.0): 9,\n",
       " ('lizaminnelli', 1.0): 1,\n",
       " ('upcom', 1.0): 2,\n",
       " ('uk', 1.0): 4,\n",
       " ('üòÑ', 1.0): 3,\n",
       " ('singl', 1.0): 4,\n",
       " ('hill', 1.0): 4,\n",
       " ('everi', 1.0): 23,\n",
       " ('beat', 1.0): 7,\n",
       " ('wrong', 1.0): 9,\n",
       " ('readi', 1.0): 22,\n",
       " ('natur', 1.0): 1,\n",
       " ('pefumeri', 1.0): 1,\n",
       " ('workshop', 1.0): 2,\n",
       " ('neal', 1.0): 1,\n",
       " ('yard', 1.0): 1,\n",
       " ('covent', 1.0): 1,\n",
       " ('tomorrow', 1.0): 31,\n",
       " ('fback', 1.0): 26,\n",
       " ...}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extracting the features\n",
    "\n",
    "* Given a list of tweets, we will extract the features and store them in a matrix. we will extract two features.\n",
    "    * The first feature is the number of positive words in a tweet.\n",
    "    * The second feature is the number of negative words in a tweet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(tweet, freqs):\n",
    "    '''\n",
    "    Input: \n",
    "        tweet: a list of words for one tweet\n",
    "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
    "    Output: \n",
    "        x: a feature vector of dimension (1,3)\n",
    "    '''\n",
    "    word_list = process_tweet(tweet)\n",
    "    x = np.zeros((1, 3)) \n",
    "    \n",
    "    #bias term set to 1\n",
    "    x[0,0] = 1 \n",
    "    \n",
    "    for word in word_list:\n",
    "        # increment the word count for the positive label 1\n",
    "        x[0,1] += freqs.get((word,1),0)\n",
    "        # increment the word count for the negative label 0\n",
    "        x[0,2] += freqs.get((word,0),0)\n",
    "    assert(x.shape == (1, 3))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cost after training is 0.24216529.\n",
      "The resulting vector of weights is [7e-08, 0.0005239, -0.00055517]\n"
     ]
    }
   ],
   "source": [
    "# collect the features 'x' and stack them into a matrix 'X'\n",
    "X = np.zeros((len(train_x), 3))\n",
    "for i in range(len(train_x)):\n",
    "    X[i, :]= extract_features(train_x[i], freqs)\n",
    "\n",
    "# training labels corresponding to X\n",
    "Y = train_y\n",
    "\n",
    "# Apply gradient descent\n",
    "J, theta = compute_gradient_descent(X, Y, np.zeros((3, 1)), 1e-9, 1500)\n",
    "print(f\"The cost after training is {J:.8f}.\")\n",
    "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(theta)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(tweet, freqs, weight):\n",
    "    x = extract_features(tweet, freqs)\n",
    "    z = np.dot(x, weight)\n",
    "    pred = compute_sigmoid(z)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the model on the following example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "such a beautiful movie -> 0.504373\n",
      "she's pretty -> 0.500561\n",
      "I hate pizza. -> 0.494093\n",
      "great -> 0.515464\n",
      "Awful -> 0.496098\n",
      "oh my god, that is horrible -> 0.493987\n"
     ]
    }
   ],
   "source": [
    "for tweet in ['such a beautiful movie', \"she's pretty\", 'I hate pizza.', 'great', 'Awful', 'oh my god, that is horrible']:\n",
    "    print( '%s -> %f' % (tweet, predict(tweet, freqs, theta)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(test_x, freqs, weight):\n",
    "    \"\"\"\n",
    "    This function returns the predicted labels (0,1)\n",
    "    \"\"\"\n",
    "    predicted_labels = []\n",
    "    score = []\n",
    "    for tweet in test_x:\n",
    "        pred = predict(tweet, freqs, weight)\n",
    "        score.append(pred)\n",
    "        \n",
    "        if pred > 0.5 :\n",
    "            predicted_labels.append(1)\n",
    "        else :\n",
    "            predicted_labels.append(0)\n",
    "            \n",
    "    return predicted_labels, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = get_labels(test_x, freqs, theta)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = list(np.squeeze(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_classification_report(observed, predicted, column_name):\n",
    "    df_metrics_report = pd.DataFrame(columns = [column_name], index = ['Accuracy', 'F1', 'Precision','Recall'])\n",
    "    df_metrics_report[column_name]['Accuracy'] = metrics.accuracy_score(observed, predicted)\n",
    "    df_metrics_report[column_name]['F1'] = metrics.f1_score(observed, predicted)\n",
    "    df_metrics_report[column_name]['Precision'] = metrics.precision_score(observed, predicted)\n",
    "    df_metrics_report[column_name]['Recall'] = metrics.recall_score(observed, predicted)\n",
    "    return df_metrics_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_performance = metrics_classification_report(true_labels, predicted_labels, 'Metrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.995000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1</th>\n",
       "      <td>0.994990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.996988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.993000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Metrics\n",
       "Accuracy   0.995000\n",
       "F1         0.994990\n",
       "Precision  0.996988\n",
       "Recall     0.993000"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's predict our own tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['realli', 'enjoy', 'movi', 'amaz']\n",
      "[[0.50441411]]\n",
      "Positive sentiment\n"
     ]
    }
   ],
   "source": [
    "# Feel free to change the tweet below\n",
    "my_tweet = 'I really enjoyed the movie. It was amazing'\n",
    "print(process_tweet(my_tweet))\n",
    "y_hat = predict(my_tweet, freqs, theta)\n",
    "print(y_hat)\n",
    "if y_hat > 0.5:\n",
    "    print('Positive sentiment')\n",
    "else: \n",
    "    print('Negative sentiment')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
